---
title: "p8105_hw5_ltc2121"
output: github_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Data import and manipulation 

```{r}
full_df = 
  tibble(
    files = list.files("data/prob1/"),
    path = str_c("data/prob1/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

Tidy data

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Plot of observations over time 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2

Import raw data and exploration

```{r}
raw_washp = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(state = case_when(city == 'Tulsa' ~ 'OK', 
                           TRUE ~ state),
    city_state = str_c(city, state, sep = ", ")) 

hom_df = raw_washp %>% 
  group_by(city_state) %>% 
  summarise(total_hom = n(), 
            unsolved_hom = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) 
hom_df %>% 
  knitr::kable()

```

This dataframe contains `r nrow(raw_washp)` rows and `r ncol(raw_washp)` columns. Each row represents an individual criminal homicide. The data includes information from `r raw_washp %>% select(city_state) %>% distinct %>% count` cities across the United States. The other key variables include details on victim name, age, race and sex. Another important notice about the dataframe was that the city Tulsa had been incorrectly matched with the state AK and needed to be corrected to be matched to the state OK. 


Making a Baltimore dataframe & prop.test 
```{r}
balti_df = hom_df %>% 
  filter(city_state == "Baltimore, MD")

test_res = prop.test(
    x = balti_df %>% pull(unsolved_hom),
    n = balti_df %>% pull(total_hom),
    alternative = c("two.sided"),
    conf.level = 0.95, 
    correct = TRUE) %>% 
  broom::tidy() %>% 
  mutate(conf.low = round(conf.low, 4),
         conf.high = round(conf.high, 4),
    CI = str_c(conf.low, conf.high, sep = "-")) %>% 
  select(estimate, CI)

```

Making function to estimate the proportion of homicides that are unsolved

```{r}
hom_prop = function(df) {
  
 test_res = prop.test(
    x = df %>% pull(unsolved_hom),
    n = df %>% pull(total_hom),
    alternative = c("two.sided"),
    conf.level = 0.95, 
    correct = TRUE) %>% 
  broom::tidy() %>% 
  mutate(conf.low = round(conf.low, 4),
         conf.high = round(conf.high, 4),
    CI = str_c(conf.low, conf.high, sep = "-")) %>% 
  select(estimate, CI)

}

```

Mapping function across all cities 

```{r}
hom_df_nest = 
  hom_df %>% 
  nest(data = total_hom:unsolved_hom)

hom_df_unnest = hom_df_nest %>% 
  mutate(
    et_CI = map(data, hom_prop)) %>%  
   unnest(et_CI)
```

Estimate of proportion plot

```{r}
hom_df_unnest %>% 
  separate(CI, c("low", "high"), "-") %>% 
  mutate(low = as.numeric(low), 
         high = as.numeric(high),
    ci = high - low, 
    city_state = fct_reorder(city_state, estimate)) %>% 
ggplot() +
  geom_point(aes(x = city_state, y = estimate)) +
  geom_errorbar( aes(x = city_state, ymin = estimate - ci, ymax = estimate + ci)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1, size = 6)) +
   labs(
    title = "Estimated Proportion of Unsolved Homicides",
    x = "US City",
    y = "Proportion")
```

## Problem 3

Making t.test function and testing function
```{r}

sim_ttest = function(mu) {
  
  x = rnorm(n = 30, mean = mu, sd = 5)

  test_results = t.test(x)
  
  test_results %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
}

sim_ttest(mu = 0)

```

Simulation 1: mu = 0 for 5000 iterations

```{r}
sim_res_df_1 = 
  expand_grid(
    true_mean = 0,
    iteration = 1:5000  
  ) %>% 
  mutate(
    estimate_df = map(true_mean, sim_ttest)
  ) 

sim_res_df_1 %>% 
  unnest(estimate_df) %>% 
  view()
```

Simulation 2: mu = {1,2,3,4,5,6} for 5000 iterations

```{r}
sim_res_df_2 = 
  expand_grid(
    true_mean = c(1, 2, 3, 4, 5, 6),
    iteration = 1:5000  
  ) %>% 
  mutate(
    estimate_df = map(true_mean, sim_ttest)
  ) 

```

First plot 

```{r}
plot_1 = 
  sim_res_df_2 %>% 
  unnest(estimate_df) %>%
  mutate(null = if_else(p.value < 0.05, "reject", 
                        if_else(p.value >= 0.05, "not_reject", NA_character_))) %>% 
  group_by(true_mean, null) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n)) %>% 
  filter(null == "reject") %>% 
  ggplot(aes(x = true_mean, y = prop)) +
  geom_bar(stat = "identity", aes( fill = true_mean)) + 
    labs(
    x = "Value of True Mean", 
    y = "Proportion", 
    title = "Proportion of Occurances with Rejected Null Hypothesis") +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6))

plot_1
```

There is a relationship shown between effect size and power in the graph that compares the proportion of times that the null hypothesis was rejected between different true mean values. This when mu is increasing while the standard deviation remains unchanged, the effect size is increasing. Statistical power is defines as the probability to correctly reject the null and relies on sample size and effect size. Therefore, an increase in effect size will result in an increase in power. This shown through the increased proportion of times that the null hypothesis was rejected as true mean values increase. 

Second Plot

```{r}
df1_plot_2 = 
  sim_res_df_2 %>% 
  unnest(estimate_df) %>%
  mutate(null = if_else(p.value < 0.05, "reject", 
                        if_else(p.value >= 0.05, "not_reject", NA_character_))) %>% 
  group_by(true_mean) %>%
  summarise(mean_mu = mean(estimate)) 

df2_plot_2 = 
  sim_res_df_2 %>% 
  unnest(estimate_df) %>%
  mutate(null = if_else(p.value < 0.05, "reject", 
                        if_else(p.value >= 0.05, "not_reject", NA_character_))) %>% 
  group_by(true_mean, null) %>%
  summarise(mean_mu = mean(estimate)) %>%
  filter(null == "reject") 

plot_2 = 
  ggplot() +
  geom_point(df1_plot_2, mapping = aes(x = true_mean, y = mean_mu, color = "blue")) +
  geom_point(df2_plot_2, mapping = aes(x = true_mean, y = mean_mu, color = "orange")) + 
  labs(
    x = "Value of True Mean", 
    y = "Average Estimate", 
    title = "Average Estimates of Mu hat for True Mean", 
    color = "Null Hypothesis Test Results") +
  scale_color_hue(labels = c("Both", "Only Reject")) +
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6)) +
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5, 6))

plot_2
```

This graph overlays all sample averages with the sample averages of across tests for which the null is rejected for each true mean. It is shown that the sample averages of across tests for which the null is rejected is not  equal to the true mean for lower true mean values. This is because the lower true mean tests had a lower power associated with the estimate results. Therefore the estimates that resulted in a rejection of the null hypothesis from these test were notably different from the true mean value. 
